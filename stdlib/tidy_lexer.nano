# Lexer-Based NanoLang Formatter
# Processes tokens for better formatting than char-by-char

# Token types (simplified for formatting)
enum TokenType {
    WORD = 0,       # Identifiers, keywords
    LPAREN = 1,     # (
    RPAREN = 2,     # )
    LBRACE = 3,     # {
    RBRACE = 4,     # }
    NEWLINE = 5,    #  \n
    SPACE = 6,      # whitespace
    OPERATOR = 7,   # : -> = + - etc
    COMMENT = 8,    # # comment
    STRING = 9      # "string literal"
}

# Simple token structure
struct Token {
    type: int,      # TokenType
    text: string
}

# Helper: Create a token
fn make_token(tok_type: int, tok_text: string) -> Token {
    # NanoLang already supports named struct literals!
    # Syntax: StructName { field: value, field: value }
    return Token { type: tok_type, text: tok_text }
}

# Tokenize source code into array of tokens
pub fn tokenize(source: string) -> array<Token> {
    let mut tokens: array<Token> = (array_new 0 (make_token 0 ""))
    let mut i: int = 0
    let len: int = (str_length source)
    
    while (< i len) {
        let c: string = (str_substring source i 1)
        
        # Skip carriage returns
        if (str_equals c "\r") {
            set i (+ i 1)
        } else {
            # Newline
            if (str_equals c "\n") {
                let token: Token = (make_token TokenType.NEWLINE "\n")
                set tokens (array_push tokens token)
                set i (+ i 1)
            } else {
                # String literal
                if (str_equals c "\"") {
                    let mut str_text: string = "\""
                    set i (+ i 1)
                    while (and (< i len) (not (str_equals (str_substring source i 1) "\""))) {
                        set str_text (+ str_text (str_substring source i 1))
                        set i (+ i 1)
                    }
                    if (< i len) {
                        set str_text (+ str_text "\"")
                        set i (+ i 1)
                    } else {}
                    let token: Token = (make_token TokenType.STRING str_text)
                    set tokens (array_push tokens token)
                } else {
                    # Comment
                    if (str_equals c "#") {
                        let mut comment_text: string = "#"
                        set i (+ i 1)
                        while (and (< i len) (not (str_equals (str_substring source i 1) "\n"))) {
                            set comment_text (+ comment_text (str_substring source i 1))
                            set i (+ i 1)
                        }
                        let token: Token = (make_token TokenType.COMMENT comment_text)
                        set tokens (array_push tokens token)
                    } else {
                        # Whitespace
                        if (or (str_equals c " ") (str_equals c "\t")) {
                            let token: Token = (make_token TokenType.SPACE " ")
                            set tokens (array_push tokens token)
                            set i (+ i 1)
                        } else {
                            # Single-char tokens
                            if (str_equals c "(") {
                                let token: Token = (make_token TokenType.LPAREN "(")
                                set tokens (array_push tokens token)
                                set i (+ i 1)
                            } else {
                                if (str_equals c ")") {
                                    let token: Token = (make_token TokenType.RPAREN ")")
                                    set tokens (array_push tokens token)
                                    set i (+ i 1)
                                } else {
                                    if (str_equals c "{") {
                                        let token: Token = (make_token TokenType.LBRACE "{")
                                        set tokens (array_push tokens token)
                                        set i (+ i 1)
                                    } else {
                                        if (str_equals c "}") {
                                            let token: Token = { type: TokenType.RBRACE, text: "}" }
                                            set tokens (array_push tokens token)
                                            set i (+ i 1)
                                        } else {
                                            # Operators: : -> = + - * / etc
                                            if (or (str_equals c ":") (or (str_equals c "=") (or (str_equals c "+") (or (str_equals c "-") (or (str_equals c "*") (or (str_equals c "/") (or (str_equals c ">") (or (str_equals c "<") (str_equals c ","))))))))) {
                                                # Check for multi-char operators like ->
                                                if (and (str_equals c "-") (and (< (+ i 1) len) (str_equals (str_substring source (+ i 1) 1) ">"))) {
                                                    let token: Token = (make_token TokenType.OPERATOR "->")
                                                    set tokens (array_push tokens token)
                                                    set i (+ i 2)
                                                } else {
                                                    let token: Token = (make_token TokenType.OPERATOR c)
                                                    set tokens (array_push tokens token)
                                                    set i (+ i 1)
                                                }
                                            } else {
                                                # Word (identifier, keyword, number)
                                                let mut word: string = ""
                                                while (and (< i len) (is_word_char (str_substring source i 1))) {
                                                    set word (+ word (str_substring source i 1))
                                                    set i (+ i 1)
                                                }
                                                if (> (str_length word) 0) {
                                                    let token: Token = (make_token TokenType.WORD word)
                                                    set tokens (array_push tokens token)
                                                } else {
                                                    set i (+ i 1)  # Skip unknown char
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
    
    return tokens
}

# Check if character is part of a word
fn is_word_char(c: string) -> bool {
    # Letters, digits, underscore
    if (str_equals c "_") {
        return true
    } else {}
    
    # Check if it's NOT a special character
    let is_special: bool = (or (str_equals c " ") (or (str_equals c "\t") (or (str_equals c "\n") (or (str_equals c "(") (or (str_equals c ")") (or (str_equals c "{") (or (str_equals c "}") (or (str_equals c ":") (or (str_equals c "=") (or (str_equals c "+") (or (str_equals c "-") (or (str_equals c "*") (or (str_equals c "/") (or (str_equals c ">") (or (str_equals c "<") (or (str_equals c ",") (or (str_equals c "\"") (str_equals c "#"))))))))))))))))))
    return (not is_special)
}

# Format tokens with proper indentation
pub fn format_tokens(tokens: array<Token>) -> string {
    let mut result: string = ""
    let mut indent_level: int = 0
    let mut at_line_start: bool = true
    let indent_size: int = 4
    let mut i: int = 0
    let count: int = (array_length tokens)
    
    while (< i count) {
        let token: Token = (at tokens i)
        
        if (== token.type TokenType.LBRACE) {
            set result (+ result " {\n")
            set indent_level (+ indent_level 1)
            set at_line_start true
        } else {
            if (== token.type TokenType.RBRACE) {
                set indent_level (- indent_level 1)
                if (< indent_level 0) {
                    set indent_level 0
                } else {}
                set result (+ result (make_indent indent_level indent_size))
                set result (+ result "}\n")
                set at_line_start true
            } else {
                if (== token.type TokenType.NEWLINE) {
                    set result (+ result "\n")
                    set at_line_start true
                } else {
                    if (== token.type TokenType.SPACE) {
                        if (not at_line_start) {
                            set result (+ result " ")
                        } else {}
                    } else {
                        # Regular token (word, operator, paren, etc.)
                        if at_line_start {
                            set result (+ result (make_indent indent_level indent_size))
                            set at_line_start false
                        } else {}
                        set result (+ result token.text)
                    }
                }
            }
        }
        
        set i (+ i 1)
    }
    
    return result
}

# Helper: Create indentation
fn make_indent(level: int, size: int) -> string {
    let total: int = (* level size)
    let mut result: string = ""
    let mut i: int = 0
    while (< i total) {
        set result (+ result " ")
        set i (+ i 1)
    }
    return result
}

# Main API: Format NanoLang source code using lexer
pub fn format_lexer(source: string) -> string {
    let tokens: array<Token> = (tokenize source)
    return (format_tokens tokens)
}

shadow is_word_char {
    assert (is_word_char "a")
    assert (is_word_char "Z")
    assert (is_word_char "_")
    assert (is_word_char "0")
    assert (not (is_word_char " "))
    assert (not (is_word_char "("))
}

shadow make_indent {
    assert (str_equals (make_indent 0 4) "")
    assert (str_equals (make_indent 1 4) "    ")
}

shadow tokenize {
    let code: string = "fn test() { }"
    let tokens: array<Token> = (tokenize code)
    assert (> (array_length tokens) 0)
}

shadow format_lexer {
    let code: string = "fn test() -> int { return 42 }"
    let formatted: string = (format_lexer code)
    assert (> (str_length formatted) 0)
    assert (str_contains formatted "{")
}

fn main() -> int {
    let code: string = "fn hello() -> int { let x: int = 42 return x }"
    (println "=== Original ===")
    (println code)
    (println "")
    (println "=== Lexer-based formatting ===")
    let formatted: string = (format_lexer code)
    (println formatted)
    return 0
}

shadow main { assert (== (main) 0) }

