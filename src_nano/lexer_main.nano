/* Backwards-compatible wrapper that exposes tokenize(source) from the lexer lib. */

import "src_nano/compiler/ir.nano"
import "src_nano/compiler/lexer.nano"
from "src_nano/compiler/diagnostics.nano" import diag_list_new

fn tokenize(source: string) -> List<LexerToken> {
    let diags: List<CompilerDiagnostic> = (diag_list_new)
    return (tokenize_string source "test.nano" diags)
}

shadow tokenize {
    let source1: string = "fn main() -> int { return 0 }"
    let tokens1: List<LexerToken> = (tokenize source1)
    assert (> (list_LexerToken_length tokens1) 0)
}

pub fn lex_phase_run(source: string, file_name: string) -> LexPhaseOutput {
    let diagnostics: List<CompilerDiagnostic> = (diag_list_new)
    let tokens: List<LexerToken> = (tokenize_string source file_name diagnostics)
    let token_count: int = (list_LexerToken_length tokens)
    let had_error: bool = (diag_list_has_errors diagnostics)
    
    if had_error {
        (println "Lexer detected errors!")
    } else {
        (print "")
    }
    
    return LexPhaseOutput {
        tokens: tokens,
        token_count: token_count,
        diagnostics: diagnostics,
        had_error: had_error
    }
}

shadow lex_phase_run {
    let output: LexPhaseOutput = (lex_phase_run "fn main() -> int { return 0 }" "test.nano")
    assert (== output.had_error false)
    assert (> output.token_count 0)
}

