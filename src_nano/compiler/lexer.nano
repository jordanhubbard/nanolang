/* =============================================================================
 * Self-hosted lexer (library)
 * =============================================================================
 * Public API:
 *   - tokenize_string(source) -> List<LexerToken>
 *   - tokenize_file(path) -> List<LexerToken>
 */

import "src_nano/compiler/ir.nano"

fn is_identifier_start(c: int) -> bool {
    return (or (or (and (>= c 65) (<= c 90)) (and (>= c 97) (<= c 122))) (== c 95))
}

fn is_identifier_char(c: int) -> bool {
    return (or (is_identifier_start c) (and (>= c 48) (<= c 57)))
}

fn is_whitespace_char(c: int) -> bool {
    return (or (or (or (== c 32) (== c 9)) (== c 10)) (== c 13))
}

fn is_digit_char(c: int) -> bool {
    return (and (>= c 48) (<= c 57))
}

fn keyword_group1(s: string) -> int {
    if (== s "module") { return LexerTokenType.TOKEN_MODULE } else {
        if (== s "pub") { return LexerTokenType.TOKEN_PUB } else {
            if (== s "from") { return LexerTokenType.TOKEN_FROM } else {
                if (== s "use") { return LexerTokenType.TOKEN_USE } else {
                    return -1
                }
            }
        }
    }
}

fn keyword_group2(s: string) -> int {
    if (== s "extern") { return LexerTokenType.TOKEN_EXTERN } else {
        if (== s "fn") { return LexerTokenType.TOKEN_FN } else {
            if (== s "let") { return LexerTokenType.TOKEN_LET } else {
                if (== s "mut") { return LexerTokenType.TOKEN_MUT } else {
                    if (== s "set") { return LexerTokenType.TOKEN_SET } else {
                        if (== s "if") { return LexerTokenType.TOKEN_IF } else {
                            if (== s "else") { return LexerTokenType.TOKEN_ELSE } else {
                                if (== s "while") { return LexerTokenType.TOKEN_WHILE } else {
                                    if (== s "for") { return LexerTokenType.TOKEN_FOR } else {
                                        if (== s "in") { return LexerTokenType.TOKEN_IN } else {
                                            if (== s "return") { return LexerTokenType.TOKEN_RETURN } else {
                                                if (== s "assert") { return LexerTokenType.TOKEN_ASSERT } else {
                                                    if (== s "shadow") { return LexerTokenType.TOKEN_SHADOW } else {
                                                        if (== s "array") { return LexerTokenType.TOKEN_ARRAY } else {
                                                            if (== s "struct") { return LexerTokenType.TOKEN_STRUCT } else {
                                                                if (== s "enum") { return LexerTokenType.TOKEN_ENUM } else {
                                                                    if (== s "union") { return LexerTokenType.TOKEN_UNION } else {
                                                                        if (== s "match") { return LexerTokenType.TOKEN_MATCH } else {
                                                                            if (== s "import") { return LexerTokenType.TOKEN_IMPORT } else {
                                                                                if (== s "as") { return LexerTokenType.TOKEN_AS } else {
                                                if (== s "opaque") { return LexerTokenType.TOKEN_OPAQUE } else {
                                                    if (== s "unsafe") { return LexerTokenType.TOKEN_UNSAFE } else {
                                                        if (== s "true") { return LexerTokenType.TOKEN_TRUE } else {
                                                            if (== s "false") { return LexerTokenType.TOKEN_FALSE } else {
                                                                return -1
                                                            }
                                                        }
                                                    }
                                                }
                                                                                }
                                                                            }
                                                                        }
                                                                    }
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                }
            }
        }
    }
}

fn keyword_group3(s: string) -> int {
    if (== s "int") { return LexerTokenType.TOKEN_TYPE_INT } else {
        if (== s "float") { return LexerTokenType.TOKEN_TYPE_FLOAT } else {
            if (== s "bool") { return LexerTokenType.TOKEN_TYPE_BOOL } else {
                if (== s "string") { return LexerTokenType.TOKEN_TYPE_STRING } else {
                    if (== s "bstring") { return LexerTokenType.TOKEN_TYPE_BSTRING } else {
                        if (== s "void") { return LexerTokenType.TOKEN_TYPE_VOID } else {
                            return -1
                        }
                    }
                }
            }
        }
    }
}

fn keyword_group4(s: string) -> int {
    if (== s "and") { return LexerTokenType.TOKEN_AND } else {
        if (== s "or") { return LexerTokenType.TOKEN_OR } else {
            if (== s "not") { return LexerTokenType.TOKEN_NOT } else {
                return -1
            }
        }
    }
}

fn keyword_or_identifier(s: string) -> int {
    let t1: int = (keyword_group1 s)
    if (!= t1 -1) {
        return t1
    } else {
        let t2: int = (keyword_group2 s)
        if (!= t2 -1) {
            return t2
        } else {
            let t3: int = (keyword_group3 s)
            if (!= t3 -1) {
                return t3
            } else {
                let t4: int = (keyword_group4 s)
                if (!= t4 -1) {
                    return t4
                } else {
                    return LexerTokenType.TOKEN_IDENTIFIER
                }
            }
        }
    }
}

fn push_tok(tokens: List<LexerToken>, token_type: int, value: string, line: int, column: int) -> void {
    let tok: LexerToken = LexerToken { token_type: token_type, value: value, line: line, column: column }
    (list_LexerToken_push tokens tok)
}

fn process_string(source: string, i: int, len: int, tokens: List<LexerToken>, line: int, column: int) -> int {
    let mut pos: int = (+ i 1)
    let start: int = pos
    while (and (< pos len) (!= (char_at source pos) 34)) {
        if (and (== (char_at source pos) 92) (< (+ pos 1) len)) {
            set pos (+ pos 2)
        } else {
            set pos (+ pos 1)
        }
    }
    if (>= pos len) {
        return -1
    } else {
        let str_length: int = (- pos start)
        let str_value: string = (str_substring source start str_length)
        (push_tok tokens LexerTokenType.TOKEN_STRING str_value line column)
        return (+ pos 1)
    }
}

fn process_number(source: string, i: int, len: int, tokens: List<LexerToken>, line: int, column: int) -> int {
    let mut pos: int = i
    let start: int = pos
    if (== (char_at source pos) 45) {
        set pos (+ pos 1)
    } else {
        (print "")
    }

    while (and (< pos len) (is_digit_char (char_at source pos))) {
        set pos (+ pos 1)
    }

    if (and (and (< pos len) (== (char_at source pos) 46)) (and (< (+ pos 1) len) (is_digit_char (char_at source (+ pos 1))))) {
        set pos (+ pos 1)
        while (and (< pos len) (is_digit_char (char_at source pos))) {
            set pos (+ pos 1)
        }
        let float_length: int = (- pos start)
        let float_value: string = (str_substring source start float_length)
        (push_tok tokens LexerTokenType.TOKEN_FLOAT float_value line column)
        return pos
    } else {
        let num_length: int = (- pos start)
        let num_value: string = (str_substring source start num_length)
        (push_tok tokens LexerTokenType.TOKEN_NUMBER num_value line column)
        return pos
    }
}

fn process_identifier(source: string, i: int, len: int, tokens: List<LexerToken>, line: int, column: int) -> int {
    let mut pos: int = i
    let start: int = pos
    while (and (< pos len) (is_identifier_char (char_at source pos))) {
        set pos (+ pos 1)
    }
    let id_length: int = (- pos start)
    let id_value: string = (str_substring source start id_length)
    let token_type: int = (keyword_or_identifier id_value)
    (push_tok tokens token_type id_value line column)
    return pos
}

fn process_char_literal(source: string, i: int, len: int, tokens: List<LexerToken>, line: int, column: int) -> int {
    let mut pos: int = (+ i 1)
    if (>= pos len) {
        return -1
    } else {
        (print "")
    }

    let mut ch: int = (char_at source pos)
    if (== ch 92) {
        set pos (+ pos 1)
        if (>= pos len) {
            return -1
        } else {
            (print "")
        }
        let esc: int = (char_at source pos)
        if (== esc 110) { set ch 10 } else {
        if (== esc 116) { set ch 9 } else {
        if (== esc 114) { set ch 13 } else {
        if (== esc 48) { set ch 0 } else {
        if (== esc 92) { set ch 92 } else {
        if (== esc 39) { set ch 39 } else {
        if (== esc 34) { set ch 34 } else {
            set ch esc
        }}}}}}}
    } else {
        (print "")
    }

    set pos (+ pos 1)
    if (>= pos len) {
        return -1
    } else {
        (print "")
    }

    if (!= (char_at source pos) 39) {
        return -1
    } else {
        let value_str: string = (int_to_string ch)
        (push_tok tokens LexerTokenType.TOKEN_NUMBER value_str line column)
        return (+ pos 1)
    }
}

pub fn tokenize_string(source: string) -> List<LexerToken> {
    let mut tokens: List<LexerToken> = (list_LexerToken_new)
    let mut i: int = 0
    let len: int = (str_length source)
    let mut line: int = 1
    let mut column: int = 1
    let mut line_start: int = 0

    while (< i len) {
        let c: int = (char_at source i)

        if (is_whitespace_char c) {
            if (== c 10) {
                set line (+ line 1)
                set line_start (+ i 1)
                set column 1
            } else {
                set column (+ column 1)
            }
            set i (+ i 1)
        } else {
            /* Line comment */
            if (== c 35) {
                while (and (< i len) (!= (char_at source i) 10)) {
                    set i (+ i 1)
                }
            } else {
                /* Block comment */
                if (and (== c 47) (and (< (+ i 1) len) (== (char_at source (+ i 1)) 42))) {
                    set i (+ i 2)
                    let mut done_comment: bool = false
                    while (and (< i len) (not done_comment)) {
                        let cc: int = (char_at source i)
                        if (== cc 10) {
                            set line (+ line 1)
                            set line_start (+ i 1)
                        } else {
                            (print "")
                        }
                        if (and (== cc 42) (and (< (+ i 1) len) (== (char_at source (+ i 1)) 47))) {
                            set i (+ i 2)
                            set done_comment true
                        } else {
                            set i (+ i 1)
                        }
                    }
                } else {
                    set column (- (+ i 1) line_start)

                    if (and (< (+ i 1) len) (and (== c 58) (== (char_at source (+ i 1)) 58))) {
                        (push_tok tokens LexerTokenType.TOKEN_DOUBLE_COLON "" line column)
                        set i (+ i 2)
                    } else {
                    if (and (< (+ i 1) len) (and (== c 45) (== (char_at source (+ i 1)) 62))) {
                        (push_tok tokens LexerTokenType.TOKEN_ARROW "" line column)
                        set i (+ i 2)
                    } else {
                    if (and (< (+ i 1) len) (and (== c 61) (== (char_at source (+ i 1)) 62))) {
                        (push_tok tokens LexerTokenType.TOKEN_ARROW "" line column)
                        set i (+ i 2)
                    } else {
                    if (and (< (+ i 1) len) (and (== c 61) (== (char_at source (+ i 1)) 61))) {
                        (push_tok tokens LexerTokenType.TOKEN_EQ "" line column)
                        set i (+ i 2)
                    } else {
                    if (and (< (+ i 1) len) (and (== c 33) (== (char_at source (+ i 1)) 61))) {
                        (push_tok tokens LexerTokenType.TOKEN_NE "" line column)
                        set i (+ i 2)
                    } else {
                    if (and (< (+ i 1) len) (and (== c 60) (== (char_at source (+ i 1)) 61))) {
                        (push_tok tokens LexerTokenType.TOKEN_LE "" line column)
                        set i (+ i 2)
                    } else {
                    if (and (< (+ i 1) len) (and (== c 62) (== (char_at source (+ i 1)) 61))) {
                        (push_tok tokens LexerTokenType.TOKEN_GE "" line column)
                        set i (+ i 2)
                    } else {
                        /* Single '=' assignment (but not '==', '=>') */
                        if (== c 61) {
                            (push_tok tokens LexerTokenType.TOKEN_ASSIGN "" line column)
                            set i (+ i 1)
                        } else {
                            let mut new_pos: int = -1
                            if (== c 39) {
                                set new_pos (process_char_literal source i len tokens line column)
                            } else {
                                if (== c 34) {
                                    set new_pos (process_string source i len tokens line column)
                                } else {
                                    if (or (is_digit_char c) (and (and (== c 45) (< (+ i 1) len)) (is_digit_char (char_at source (+ i 1))))) {
                                        set new_pos (process_number source i len tokens line column)
                                    } else {
                                        if (is_identifier_start c) {
                                            set new_pos (process_identifier source i len tokens line column)
                                        } else {
                                            if (== c 40) {
                                                (push_tok tokens LexerTokenType.TOKEN_LPAREN "" line column)
                                                set new_pos (+ i 1)
                                            } else {
                                                if (== c 41) {
                                                    (push_tok tokens LexerTokenType.TOKEN_RPAREN "" line column)
                                                    set new_pos (+ i 1)
                                                } else {
                                                    if (== c 123) {
                                                        (push_tok tokens LexerTokenType.TOKEN_LBRACE "" line column)
                                                        set new_pos (+ i 1)
                                                    } else {
                                                        if (== c 125) {
                                                            (push_tok tokens LexerTokenType.TOKEN_RBRACE "" line column)
                                                            set new_pos (+ i 1)
                                                        } else {
                                                            if (== c 91) {
                                                                (push_tok tokens LexerTokenType.TOKEN_LBRACKET "" line column)
                                                                set new_pos (+ i 1)
                                                            } else {
                                                                if (== c 93) {
                                                                    (push_tok tokens LexerTokenType.TOKEN_RBRACKET "" line column)
                                                                    set new_pos (+ i 1)
                                                                } else {
                                                                    if (== c 44) {
                                                                        (push_tok tokens LexerTokenType.TOKEN_COMMA "" line column)
                                                                        set new_pos (+ i 1)
                                                                    } else {
                                                                        if (== c 58) {
                                                                            (push_tok tokens LexerTokenType.TOKEN_COLON "" line column)
                                                                            set new_pos (+ i 1)
                                                                        } else {
                                                                            if (== c 46) {
                                                                                (push_tok tokens LexerTokenType.TOKEN_DOT "" line column)
                                                                                set new_pos (+ i 1)
                                                                            } else {
                                                                                if (== c 43) {
                                                                                    (push_tok tokens LexerTokenType.TOKEN_PLUS "" line column)
                                                                                    set new_pos (+ i 1)
                                                                                } else {
                                                                                    if (== c 45) {
                                                                                        (push_tok tokens LexerTokenType.TOKEN_MINUS "" line column)
                                                                                        set new_pos (+ i 1)
                                                                                    } else {
                                                                                        if (== c 42) {
                                                                                            (push_tok tokens LexerTokenType.TOKEN_STAR "" line column)
                                                                                            set new_pos (+ i 1)
                                                                                        } else {
                                                                                            if (== c 47) {
                                                                                                (push_tok tokens LexerTokenType.TOKEN_SLASH "" line column)
                                                                                                set new_pos (+ i 1)
                                                                                            } else {
                                                                                                if (== c 37) {
                                                                                                    (push_tok tokens LexerTokenType.TOKEN_PERCENT "" line column)
                                                                                                    set new_pos (+ i 1)
                                                                                                } else {
                                                                                                    if (== c 60) {
                                                                                                        (push_tok tokens LexerTokenType.TOKEN_LT "" line column)
                                                                                                        set new_pos (+ i 1)
                                                                                                    } else {
                                                                                                        if (== c 62) {
                                                                                                            (push_tok tokens LexerTokenType.TOKEN_GT "" line column)
                                                                                                            set new_pos (+ i 1)
                                                                                                        } else {
                                                                                                            set new_pos (+ i 1)
                                                                                                        }
                                                                                                    }
                                                                                                }
                                                                                            }
                                                                                        }
                                                                                    }
                                                                                }
                                                                            }
                                                                        }
                                                                    }
                                                                }
                                                            }
                                                        }
                                                    }
                                                }
                                            }
                                        }
                                    }
                                }
                            }

                            if (== new_pos -1) {
                                set i len
                            } else {
                                set i new_pos
                            }
                        }
                    }}}}}}}
                }
            }
        }
    }

    (push_tok tokens LexerTokenType.TOKEN_EOF "" line column)
    return tokens
}

pub fn tokenize_file(path: string) -> List<LexerToken> {
    let source: string = (file_read path)
    return (tokenize_string source)
}

shadow tokenize_string {
    let source: string = "import \"x.nano\" as X\nfn main() -> int { return 0 }"
    let toks: List<LexerToken> = (tokenize_string source)
    assert (> (list_LexerToken_length toks) 0)
}
