# Simplified lexer - testing incremental complexity

enum TokenType {
    TOKEN_EOF,
    TOKEN_NUMBER,
    TOKEN_STRING,
    TOKEN_IDENTIFIER,
    TOKEN_FN,
    TOKEN_LET
}

struct Token {
    type: int,
    value: string,
    line: int,
    column: int
}

fn is_digit_char(c: int) -> bool {
    return (and (>= c 48) (<= c 57))
}

shadow is_digit_char {
    assert (== (is_digit_char 48) true)
    assert (== (is_digit_char 57) true)
    assert (== (is_digit_char 65) false)
}

fn keyword_or_identifier(s: string) -> int {
    if (== s "fn") {
        return TokenType.TOKEN_FN
    } else {
        if (== s "let") {
            return TokenType.TOKEN_LET
        } else {
            return TokenType.TOKEN_IDENTIFIER
        }
    }
}

shadow keyword_or_identifier {
    assert (== (keyword_or_identifier "fn") TokenType.TOKEN_FN)
    assert (== (keyword_or_identifier "let") TokenType.TOKEN_LET)
    assert (== (keyword_or_identifier "x") TokenType.TOKEN_IDENTIFIER)
}

fn tokenize_simple(source: string) -> list_token {
    let mut tokens: list_token = (list_token_new)
    let mut i: int = 0
    let len: int = (str_length source)
    
    while (< i len) {
        let c: int = (char_at source i)
        
        if (is_digit_char c) {
            let start: int = i
            while (and (< i len) (is_digit_char (char_at source i))) {
                set i (+ i 1)
            }
            let num_value: string = (str_substring source start i)
            let tok: Token = Token { type: TokenType.TOKEN_NUMBER, value: num_value, line: 1, column: 1 }
            (list_token_push tokens tok)
        } else {
            set i (+ i 1)
        }
    }
    
    let eof_tok: Token = Token { type: TokenType.TOKEN_EOF, value: "", line: 1, column: 1 }
    (list_token_push tokens eof_tok)
    return tokens
}

shadow tokenize_simple {
    let source: string = "42 123"
    let tokens: list_token = (tokenize_simple source)
    assert (> (list_token_length tokens) 0)
}

fn main() -> int {
    let source: string = "42 123"
    let tokens: list_token = (tokenize_simple source)
    return 0
}

shadow main {
    assert (== (main) 0)
}

