# ============================================================================
# Autonomous GitHub Agent - Environment Configuration
# ============================================================================
#
# Copy this file to .env and fill in your actual values
# Then run: source .env && ./autonomous_github_agent
#

# ============================================================================
# GitHub Authentication (Required - choose ONE method)
# ============================================================================

# Method 1: Personal Access Token (recommended for getting started)
# Create at: https://github.com/settings/tokens
# Required scopes: repo (full control of private repositories)
export GITHUB_TOKEN="ghp_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"

# Method 2: GitHub App Authentication (enterprise/advanced)
# For GitHub App authentication, you need:
# 1. Create GitHub App at: https://github.com/settings/apps
# 2. Install app on your repository
# 3. Get these values:
#
# export GITHUB_APP_ID="123456"
# export GITHUB_APP_INSTALLATION_ID="12345678"
# export GITHUB_APP_PRIVATE_KEY="-----BEGIN RSA PRIVATE KEY-----
# MIIEpAIBAAKCAQEA1234567890abcdef...
# ...more key data...
# ...more key data...
# -----END RSA PRIVATE KEY-----"
#
# Note: GitHub App support requires JWT signing (coming soon)
# For now, you can generate an installation token manually:
#   gh api -X POST /app/installations/{installation_id}/access_tokens
# Then use the token value as GITHUB_TOKEN above

# ============================================================================
# GitHub Repository Configuration (Required)
# ============================================================================

# Repository owner (username or organization)
export GITHUB_OWNER="jordanhubbard"

# Repository name
export GITHUB_REPO="nanolang"

# Optional: Filter by label (leave empty to process all issues)
export GITHUB_ISSUE_LABEL="bug"
# Other examples:
# export GITHUB_ISSUE_LABEL="needs-triage"
# export GITHUB_ISSUE_LABEL="good-first-issue"

# ============================================================================
# OpenAI/LLM Configuration
# ============================================================================

# Option 1: OpenAI API (default)
export OPENAI_API_URL="https://api.openai.com/v1"
export OPENAI_API_KEY="sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
export OPENAI_MODEL="gpt-4"

# Option 2: OpenAI with GPT-3.5-turbo (faster, cheaper)
# export OPENAI_API_URL="https://api.openai.com/v1"
# export OPENAI_API_KEY="sk-proj-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx"
# export OPENAI_MODEL="gpt-3.5-turbo"

# Option 3: Ollama (local LLM - free, private, no API key needed)
# First run: ollama pull llama2
# Then start: ollama serve
# export OPENAI_API_URL="http://localhost:11434/v1"
# export OPENAI_API_KEY="dummy"  # Not needed for Ollama
# export OPENAI_MODEL="llama2"

# Option 4: LM Studio (local LLM - free, private)
# Start LM Studio server on port 1234
# export OPENAI_API_URL="http://localhost:1234/v1"
# export OPENAI_API_KEY="dummy"  # Not needed for LM Studio
# export OPENAI_MODEL="local-model"

# Option 5: Azure OpenAI
# export OPENAI_API_URL="https://your-resource.openai.azure.com/openai/deployments/your-deployment"
# export OPENAI_API_KEY="your-azure-key"
# export OPENAI_MODEL="gpt-4"

# Option 6: Custom OpenAI-compatible endpoint
# export OPENAI_API_URL="https://your-custom-endpoint.com/v1"
# export OPENAI_API_KEY="your-custom-key"
# export OPENAI_MODEL="your-model-name"

# ============================================================================
# Agent Configuration
# ============================================================================

# Agent mode: what type of analysis to perform
# Options: analyze, triage, fix, monitor
export AGENT_MODE="analyze"

# Dry run: if "true", agent will analyze but NOT post comments
# Set to "false" for production use
export AGENT_DRY_RUN="true"

# Maximum number of issues to process per run
export AGENT_MAX_ISSUES="5"

# ============================================================================
# Usage Examples
# ============================================================================
#
# 1. Dry run (safe - no changes made):
#    source .env
#    ./autonomous_github_agent
#
# 2. Production run (will post comments):
#    export AGENT_DRY_RUN="false"
#    ./autonomous_github_agent
#
# 3. Process only "bug" labeled issues:
#    export GITHUB_ISSUE_LABEL="bug"
#    ./autonomous_github_agent
#
# 4. Use local LLM (Ollama):
#    export OPENAI_API_URL="http://localhost:11434/v1"
#    export OPENAI_API_KEY="dummy"
#    export OPENAI_MODEL="llama2"
#    ./autonomous_github_agent
#
# 5. Different analysis modes:
#    export AGENT_MODE="triage"    # Suggest labels
#    export AGENT_MODE="fix"       # Suggest bug fixes
#    export AGENT_MODE="analyze"   # General analysis
#
# ============================================================================
# Troubleshooting
# ============================================================================
#
# Issue: "GITHUB_TOKEN environment variable not set"
# Solution: Make sure you exported the variable: export GITHUB_TOKEN="..."
#
# Issue: "Could not check rate limit (may be invalid token)"
# Solution: Verify your token has correct permissions and hasn't expired
#
# Issue: "LLM Error: curl failed"
# Solution: Check OPENAI_API_URL is correct and accessible
#           For local LLMs, ensure the server is running
#
# Issue: Rate limit warnings
# Solution: GitHub allows 5,000 requests/hour. Wait or use AGENT_MAX_ISSUES
#           to limit processing
#
# ============================================================================
# Security Notes
# ============================================================================
#
# - NEVER commit .env files with real credentials to git
# - Add .env to .gitignore
# - Rotate tokens periodically
# - Use GitHub App authentication for production deployments
# - For CI/CD, use GitHub Actions secrets instead of .env files
# - Minimum token scopes: repo (or public_repo for public repos only)
#
# ============================================================================
